name: pre_train
version: 1.0
description: Config of model pre-training.

# base
base_path: "/home/data_efficacy"
master_port: 2030
gpus_per_node: 8
nnodes: 1

# model
model_type: "mistral"
ckpt_name: "mistral/160M"
from_scratch: true
attn_impl: "eager"
xops_attn: true

# data
data_name: "cc-sgd100-160M-lima-lqs-t0.0-r0.9"
data_dir: "/home/data_efficacy/processed_data/pretrain/cc-sgd100-160M-lima-lqs-t0.0-r0.9"
dev_data_dir: "/home/data_efficacy/processed_data/pretrain_eval/cc-mistral-1025/dev-100K"
num_workers: 8
dev_num: 16384
bin_data: true
no_shuffle: true

# train
lr: 0.0006
lr_min: 0.00006
batch_size: 8
eval_batch_size: 64
gradient_accumulation_steps: 4
warmup_iters: 2000
scheduler_name: "cosine"
weight_decay: 0.01
clip_grad: 1.0
adam_beta: 0.9
adam_beta2: 0.98
adam_eps: 1e-6
total_iters: 100000
max_length: 1024

# runtime
do_train: true
do_valid: true
save_interval: 5000
eval_interval: 1000
log_interval: 10
mid_log_num: -1
seed: 10

# deepspeed
deepspeed: true
deepspeed_config: "/home/data_efficacy/configs/deepspeed/ds_config.json"