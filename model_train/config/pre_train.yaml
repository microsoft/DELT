name: pre_train
version: 1.0
description: Config of model pre-training.


# model
model_type: mistral
ckpt_name: mistral/160M
from_scratch: true
attn_impl: eager
xops_attn: true

# data
data_name: cc-lqs-r0.9-folding-l3
data_path: data/cc/lqs_scored_selected_r0.9_ordered_folding_l3_data.jsonl
dev_data_dir: "/home/data_efficacy/processed_data/pretrain_eval/cc-mistral-1025/dev-100K"
num_workers: 8
dev_num: 16384
json_data: true
no_shuffle: true
data_split: null

# train hp
lr: 0.0006
lr_min: 0.00006
batch_size: 8
eval_batch_size: 64
gradient_accumulation_steps: 4
warmup_iters: 2000
scheduler_name: cosine
weight_decay: 0.01
clip_grad: 1.0
adam_beta: 0.9
adam_beta2: 0.98
adam_eps: 1e-6
epochs: 1
max_length: 1024

# runtime
do_train: true
do_valid: true
save_interval: 5000
eval_interval: 1000
log_interval: 10
mid_log_num: -1
seed: 10

# deepspeed
deepspeed: true
deepspeed_config: model_train/config/ds_config.json

# wandb
wandb_name: 160M-cc-lqs-r0.9-folding-l3
wandb_group: pretrain
wandb_id: null

# others
model_parallel: false
dropout_path_rate: null