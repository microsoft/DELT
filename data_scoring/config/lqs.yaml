name: LQS
version: 1.0
description: Config of LQS method in data scoring.

full_data:
  hf_model_id: Data-Selection/BSL-160M
  output_model_path: models/mistral/160M

  type: data_processing 
  data_name: cc 
  model_path: models/mistral/160M 
  save: processed_data/data_scorer/lqs
  field: text
  max_length: 1025
  log_interval: 10000
  data_process_workers: 32
  model_type: mistral
  chunk_num_per_shard: 1000000
  max_shard_num: 10000000
  seed: 10

target_data:
  hf_data_id: GAIR/lima
  hf_data_name: plain_text
  split_name: train
  sample_size: -1
  output_data_path: data/lima/original_data.jsonl
  split_name: train

  type: data_processing
  data_name: lima
  data_path: data/lima
  field: conversations
  save: processed_data/data_scorer/lqs
  model_path: models/mistral/160M
  data_process_workers: 32
  max_length: 1025
  model_type: mistral
  seed: 10

proxy_data:
  data_path: processed_data/data_scorer/lqs/cc/mistral-1025
  proxy_num: 163840
  type: data_processing
  data_name: cc-mistral-1025
  save: processed_data/proxy
  proxy-num: 163840
  max_state: 10
  min_state: 0
  seed: 10

annotation_data:
  type: annotation_data

  # model
  model_type: mistral
  model_path: models/mistral/160M
  ckpt_name: 160M-10k
  attn_impl: eager
  fp32: true
  
  # data 
  max_state: 10
  proxy_num: 163840
  data_name: cc-lima
  data_dir: processed_data/data_scorer/lqs/cc/mistral-1025
  dev_data_dir: processed_data/data_scorer/lqs/lima/mistral-1025/dev
  proxy_data_dir: processed_data/proxy/cc-mistral-1025/163840
  bin_data: true
  no_shuffle: true
  dataset_type: lm
  train_num: null
  dev_num: null
  
  # hp
  total_iters: 100
  epochs: null
  lr: 0.008
  batch_size: 8
  grad_batch_size: 2
  proxy_batch_size: 8
  eval_batch_size: 4
  gradient_accumulation_steps: 2
  clip_grad: -1
  max_length: 1024
  num_workers: 2
  weight_decay: 0.0
  optimizer_name: sgd
  scheduler_name: constant
  warmup_iters: 0
  min_offset: 0
  min_prompt_length: 128
  max_prompt_length: 512
  from_scratch: false
  gradient_checkpointing: false

  # runtime
  save: results/annotation_data
  log_interval: 1
  eval_interval: 10
  wandb_group: annotation_data
  compute_ct_interval: 10

  model_parallel: false
  dropout_path_rate: null
  xops_attn: false
  
  # seed
  seed: 10
  seed_data: 10
  
  # wandb
  wandb_mode: disabled
  wandb_name: null

scorer_data_training:
  base-path: "/home/data_efficacy"

  type: "data-scorer"
  ckpt-name: "fairseq/125M"
  ckpt-path: "/home/data_efficacy/checkpoints/fairseq/125M/"
  model-type: "fairseq"
  data-scorer-encoding: "mean"
  data-scorer-bias: true
  data-scorer-head-type: "linear"
  # gradient-checkpointing: true

  data-dir: "/home/data_efficacy/processed_data/data_scorer_train/cc-sgd100-160M-lima-lqs-163840"
  data-name: "cc-sgd100-160M-lima-lqs-163840"
  num-workers: 0
  train-num: 163840
  dev-num: 16384

  lr: 0.0001
  batch-size: 16
  eval-batch-size: 64
  gradient-accumulation-steps: 2
  warmup-iters: 10
  scheduler-name: "cosine"
  weight-decay: 1e-2
  clip-grad: 1.0
  epochs: 5
  max-length: 1024

  do-train: true
  do-valid: true
  save-interval: -1
  eval-interval: -1
  log-interval: 1
  mid-log-num: -1
  save-path: "/home/data_efficacy/results/data_scorer"
  seed: 10

  deepspeed: true
  deepspeed-config: "/home/data_efficacy/configs/deepspeed/ds_config.json"
  
  wandb-mode: "disabled"

scorer_data_infer:
