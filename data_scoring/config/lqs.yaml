name: LQS
version: 1.0
description: Config of LQS method in data scoring.

full_data:
# 
  hf_model_id: Data-Selection/BSL-160M
  output_model_path: models/mistral/160M

# 
  type: data_processing 
  data_name: cc 
  model_path: models/mistral/160M 
  save: processed_data/data_scorer/lqs
  field: text
  max_length: 1025
  log_interval: 10000
  data_process_workers: 32
  model_type: mistral
  chunk_num_per_shard: 1000000
  max_shard_num: 10000000
  seed: 10

target_data:
# 
  hf_data_id: GAIR/lima
  hf_data_name: plain_text
  split_name: train
  sample_size: -1
  output_data_path: data/lima/original_data.jsonl
  split_name: train

# 
  type: data_processing
  data_name: lima
  data_path: data/lima
  field: conversations
  save: processed_data/data_scorer/lqs
  model_path: models/mistral/160M
  data_process_workers: 32
  max_length: 1025
  model_type: mistral
  seed: 10

proxy_data:
# 
  data_path: processed_data/data_scorer/lqs/cc/mistral-1025
  proxy_num: 163840
  type: data_processing
  data_name: cc-mistral-1025
  save: processed_data/proxy
  proxy-num: 163840
  max_state: 10
  min_state: 0
  seed: 10

annotation_data:
# 
  type: annotation_data

  # model
  model_type: mistral
  model_path: models/mistral/160M
  ckpt_name: 160M-10k
  attn_impl: eager
  fp32: true
  # data 
  max_state: 10
  proxy_num: 163840
  data_name: cc-lima
  data_dir: processed_data/data_scorer/lqs/cc/mistral-1025
  dev_data_dir: processed_data/data_scorer/lqs/lima/mistral-1025/dev
  proxy_data_dir: processed_data/proxy/cc-mistral-1025/163840
  bin_data: true
  no_shuffle: true
  dataset_type: lm
  train_num: null
  dev_num: null
  # hp
  total_iters: 100
  epochs: null
  lr: 0.008
  batch_size: 8
  grad_batch_size: 2
  proxy_batch_size: 8
  eval_batch_size: 4
  gradient_accumulation_steps: 2
  clip_grad: -1
  max_length: 1024
  num_workers: 2
  weight_decay: 0.0
  optimizer_name: sgd
  scheduler_name: constant
  warmup_iters: 0
  min_offset: 0
  min_prompt_length: 128
  max_prompt_length: 512
  from_scratch: false
  gradient_checkpointing: false
  # runtime
  save: results/annotation_data
  log_interval: 1
  eval_interval: 10
  wandb_group: annotation_data
  compute_ct_interval: 10
  model_parallel: false
  dropout_path_rate: null
  xops_attn: false
  # seed
  seed: 10
  seed_data: 10
  # wandb
  wandb_mode: disabled
  wandb_name: null
  # data prepare
  hf_model_id: KoboldAI/fairseq-dense-125M
  output_model_path: models/fairseq/125M

# 
  proxy_score_path: results/annotation_data/cc-lima/160M-10k/sgd-t100-bs8-lr0.008constant-G2-ct10
  data_scorer_tokenizer_path: models/fairseq/125M
  data_scorer_model_type: fairseq
  proxy_data_name: cc-sgd100-160M-10k-lima
  proxy_save: processed_data/data_scorer_train
  data_process_workers: 32
  chunk_num_per_shard: 1000000
  proxy_dev_num: 16384

scorer_data_training:
# 
  type: scoring_data
  ckpt_name: fairseq/125M
  model_path: models/fairseq/125M
  model_type: fairseq
  data_scorer_encoding: mean
  data_scorer_bias: true
  data_scorer_head_type: linear

  data_dir: processed_data/data_scorer_train/cc-sgd100-160M-10k-lima-163840
  data_name: cc-sgd100-160M-lima-lqs-163840
  num_workers: 0
  train_num: 163840
  dev_num: 16384

  lr: 0.0001
  lr_min: 0.0000001
  batch_size: 16
  eval_batch_size: 64
  gradient_accumulation_steps: 2
  warmup_iters: 10
  scheduler_name: "cosine"
  weight_decay: 1e-2
  clip_grad: 1.0
  epochs: 5
  total_iters: null
  max_length: 1024
  min_prompt_length: 128
  max_prompt_length: 512
  from_scratch: false
  gradient_checkpointing: false
  attn_impl: null
  fp32: false
  optimizer_name: AdamW
  adam_eps: 1e-8
  adam_beta: 0.9
  adam_beta2: 0.999

  do_train: true
  do_valid: true
  do_infer: false
  save_interval: -1
  eval_interval: -1
  log_interval: 1
  mid_log_num: -1
  save: results/data_scorer
  seed: 10
  order_seed: 10
  precompute_data_order: false
  resume_training: false
  no_shuffle: false
  start_from_global_step: null
  save_all: false

  # 
  deepspeed: true
  deepspeed_config: model_train/config/ds_config.json

  # 
  wandb_mode: disabled
  wandb_name: null
  wandb_group: null
  wandb_id: null

  #
  model_parallel: false
  dropout_path_rate: null
  xops_attn: false
  torch_compile: null

scorer_data_infer:

  # convert tokenize 
  data_name: cc
  old_model_type: mistral
  old_model_path: models/mistral/160M
  new_model_type: fairseq
  new_model_path: models/fairseq/125M
  data_path: processed_data/data_scorer/lqs/cc/mistral-1025
  save_convert_data: processed_data/data_scorer_infer
  max_length: 1024
  convert_data_process_workers: 32
  chunk_num_per_shard: 1000000
  min_state: 0

  # data infer
  type: data_scorer

  # model
  model_path: results/data_scorer/cc-sgd100-160M-lima-lqs-163840/fairseq_125M/e5-w10-bs16-lr0.0001cosine-G2/mean-bias-linear/23040
  ckpt_name: cc-160M-lima
  model_type: fairseq
  attn_impl: eager
  xops_attn: true
  torch_compile: reduce-overhead
  gradient_accumulation_steps: 1
  # data
  data_dir: processed_data/data_scorer_infer/cc/mistral-fairseq-1024
  num_workers: 0
  infer_num: 160000000
  # hp
  eval_batch_size: 128
  batch_size: 32
  clip_grad: 1 
  min_prompt_length: 128
  max_prompt_length: 512
  from_scratch: false
  gradient_checkpointing: false
  fp32: false
  # runtime
  do_train: false
  do_valid: false
  do_infer: true
  log_interval: 10
  save_interval: 2500
  save: results/data_scorer_infer/
  # seed
  seed: 10
  order_seed: 10
  # deepspeed
  deepspeed: true
  deepspeed_config: model_train/config/ds_config.json
  # wandb
  wandb_mode: disabled
  wandb_name: null
  wandb_group: null
  wandb_id: null

  #
  model_parallel: false
  dropout_path_rate: null

  # jsonl data saving
  bin_data_path: processed_data/data_scorer/lqs/cc/mistral-1025
  pt_score_path: results/data_scorer_infer/cc/cc-160M-lima
  jsonl_tokenizer_path: models/mistral/160M
  jsonl_model_type: mistral
  save_score_name: lqs_score
